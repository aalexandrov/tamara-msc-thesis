system {
    spark {
        config {
            # default slaves for wally (specified in hosts.conf)
            slaves = ${system.default.config.slaves}

            # spark-env.sh entries
            env {
                HADOOP_CONF_DIR = ${system.hadoop-2.path.config}

                # enable this if you want to use spark with native libraries
                # only use if there is a hadoop version compiled with native libraries for wally!
                # SPARK_DAEMON_JAVA_OPTS = "-Djava.library.path="${system.hadoop-2.path.home}"/lib/native"

                # wally has 16 GB per node
                SPARK_EXECUTOR_MEMORY = "15360m"
                SPARK_WORKER_MEMORY = "15360m"
            }

            # spark-defaults.conf
            defaults {
                spark.master = "spark://"${runtime.hostname}":7077"
                spark.eventLog.enabled = "true"
                spark.eventLog.dir = "file://"${system.spark.path.log}

                # tmp folder for spilling data to disk (on node-local storage)
                spark.local.dir = "/data/"${user.name}"/spark_tmp"

                # yet another conf param for the executor memory
                spark.executor.memory = "15360m"
                
                # memory of driver (e.g. to receive/gather results sets)
                spark.driver.memory = "8192m"
                
                # sets the memory limit of result sets gathered on driver to infinite
                spark.driver.maxResultSize = "0"
            }
        }
    }
}